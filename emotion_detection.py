# -*- coding: utf-8 -*-
"""Emotion Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d8R_NCkQY2A1sRItvbYBDuuppEW0n2gR

**Setting Up the kaggle Directory**
"""

!mkdir -p ./kaggle
!cp kaggle.json ~/.kaggle/

!kaggle datasets download -d msambare/fer2013

!unzip /content/fer2013.zip -d /content/

import os
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from PIL import Image

"""**Making the Directory to Save the Model**"""

os.makedirs('test_dir',exist_ok=True)

##Project Name
project_name='FER_2013_Emotion_Detection'

#Names of the Folders inside the test Directory
model_names=[
    'Custom_CNN_From_Scratch',
    'Custom_CNN_With_Augmentation'
]

base_dir='/content/'

project_dir=os.path.join(base_dir,project_name)
os.makedirs(project_dir,exist_ok=True)

for model in model_names:
  os.makedirs(os.path.join(project_dir,model),exist_ok=True)

print(f'Project directory structure created at: {project_dir}')

"""**Data Cleaning**"""

import imghdr
import cv2
image_exts=['jpeg','jpg','png']

data_dir='/content/train'

for root,dirs,files in os.walk(data_dir):
  for file in files:
    file_path=os.path.join(root,file)

  ##This is exception handling part
    try:
      #Checking the image type
      file_type=imghdr.what(file_path)
      #If having not allowed extension than removing it
      if file_type not in image_exts:
        print('Image type is not allowed')
        os.remove(file_path)
      else:
        img=cv2.imread(file_path)
  ##If invalid path than remove
    except Exception as e:
      print(f'Issu with file {file_path}. Error: {e}')
      os.remove(file_path)

"""**Data Analysis**"""

# Define a function to count the number of files (assumed to be images for this context) for each subdirectory in a given directory.
# The function returns a DataFrame with these counts, indexed by a specified set name (e.g., 'train' or 'test').
def count_files_in_subdirs(directory, set_name):
    # Initialize an empty dictionary to hold the count of files for each subdirectory.
    counts = {}

    # Iterate over each item in the given directory.
    for item in os.listdir(directory):
        # Construct the full path to the item.
        item_path = os.path.join(directory, item)

        # Check if the item is a directory.
        if os.path.isdir(item_path):
            # Count the number of files in the subdirectory and add it to the dictionary.
            counts[item] = len(os.listdir(item_path))

    # Convert the counts dictionary to a DataFrame for easy viewing and analysis.
    # The index of the DataFrame is set to the provided set name.
    df = pd.DataFrame(counts, index=[set_name])
    return df

# Paths to the training and testing directories.
train_dir = '/content/train'
test_dir = '/content/test'

# Count the files in the subdirectories of the training directory and print the result.
train_count = count_files_in_subdirs(train_dir, 'train')
print(train_count)

# Count the files in the subdirectories of the testing directory and print the result.
test_count = count_files_in_subdirs(test_dir, 'test')
print(test_count)

import seaborn as sns

sns.barplot(train_count)

sns.barplot(test_count)

"""**Plotting Images**"""

emotions = os.listdir(train_dir)
plt.figure(figsize=(15,10))

for i, emotion in enumerate(emotions, 1):
    folder = os.path.join(train_dir, emotion)
    img_path = os.path.join(folder, os.listdir(folder)[41])
    #Reading the image
    img = plt.imread(img_path)
    #(rows,columns,position from 1 to 12)
    plt.subplot(3, 4, i)
    #Plotting the image
    plt.imshow(img, cmap='gray')
    plt.title(emotion)
    plt.axis('off')

"""**Plotting Random Images**"""

def plot_images_from_directory(directory_path, class_name, num_images=9):
    # Retrieve list of all file names in the directory
    image_filenames = os.listdir(directory_path)

    # If there are fewer images than requested, we'll just show them all
    if len(image_filenames) < num_images:
        print(f"Only found {len(image_filenames)} images in {directory_path}, displaying them all.")
        num_images = len(image_filenames)

    # Randomly select 'num_images' number of file names
    selected_images = random.sample(image_filenames, num_images)

    # Plotting the images
    fig, axes = plt.subplots(3, 3, figsize=(5, 5))  # Adjust the size as needed
    axes = axes.ravel()

    for i, image_file in enumerate(selected_images):
        image_path = os.path.join(directory_path, image_file)
        image = cv2.imread(image_path)
        axes[i].imshow(image)
        axes[i].set_title(f"Image: {class_name}")
        axes[i].axis('off')  # Hide the axis

    plt.tight_layout()
    plt.show()

plot_images_from_directory('/content/train/angry','Angry')

image = '/content/train/angry/Training_10118481.jpg'

import cv2

img = cv2.imread(image) # Default load in color format.

# If the image is loaded successfully, print its pixel values
if img is not None:
    # print(img)
    print("Shape:", img.shape)
else:
    print("The image could not be loaded. Please check the path and file permissions.")

image = '/content/train/angry/Training_10118481.jpg'

import cv2

img = cv2.imread(image,cv2.IMREAD_GRAYSCALE) # Default load in color format.

# If the image is loaded successfully, print its pixel values
if img is not None:
    # print(img)
    print("Shape:", img.shape)
else:
    print("The image could not be loaded. Please check the path and file permissions.")

"""**Building CNN Model**

ImageDataGenerator
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models, regularizers, optimizers
from tensorflow.keras.applications import VGG16, ResNet50V2
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard, CSVLogger
from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dropout, Flatten, Dense, Activation, GlobalAveragePooling2D
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.optimizers import Adam, Adamax
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
from keras.utils import plot_model

# Define paths to the train and validation directories
train_data_dir = '/content/train'
test_data_dir = '/content/test'

# Set some parameters
img_width, img_height = 48, 48  # Size of images
batch_size = 64
epochs = 10
num_classes = 7  # Update this based on the number of your classes

# Rescale the pixel values (0-255) to the [0, 1] interval
data_generator = ImageDataGenerator(rescale=1./255,
                                    validation_split=0.2)

# Automatically retrieve images and their classes for train and validation sets
train_generator = data_generator.flow_from_directory(
    train_data_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='categorical',
    color_mode='grayscale',
    subset='training')

validation_generator = data_generator.flow_from_directory(
    train_data_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='categorical',
    color_mode='grayscale',
    subset='validation')

test_generator = data_generator.flow_from_directory(
    test_data_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='categorical',
    color_mode='grayscale',)
    # subset='validation')

# Accessing class labels for the training data
train_class_labels = train_generator.class_indices
print("Training class labels:", train_class_labels)

# Accessing class labels for the validation data
validation_class_labels = validation_generator.class_indices
print("Validation class labels:", validation_class_labels)

# Accessing class labels for the validation data
test_class_labels = test_generator.class_indices
print("Validation class labels:", test_class_labels)

"""CNN MODEL"""

# Initialising the CNN
model = Sequential()

# Adding convolutional layers with activations on the same line for clarity
model.add(Conv2D(32, kernel_size=(3, 3),kernel_initializer="glorot_uniform", padding='same', input_shape=(img_width, img_height, 1)))
model.add(Activation('relu'))
model.add(Conv2D(64, kernel_size=(3, 3), padding='same'))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(2, 2))
model.add(Dropout(0.25))

model.add(Conv2D(128, kernel_size=(3, 3), padding='same', kernel_regularizer=regularizers.l2(0.01)))
model.add(Activation('relu'))
model.add(Conv2D(256, kernel_size=(3, 3), kernel_regularizer=regularizers.l2(0.01)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Conv2D(512, kernel_size=(3, 3), padding='same', kernel_regularizer=regularizers.l2(0.01)))
model.add(Activation('relu'))
model.add(Conv2D(512, kernel_size=(3, 3), padding='same', kernel_regularizer=regularizers.l2(0.01)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

# Flattening and adding dense layers
model.add(Flatten())
model.add(Dense(1024))
model.add(Activation('relu'))
model.add(Dropout(0.5))

# Output layer
model.add(Dense(num_classes))
model.add(Activation('softmax'))

model.summary()

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

checkpoint_path='/content/FER_2013_Emotion_Detection/Custom_CNN_From_Scratch/'
name='Custom_CNN_Model_chkpt'
checpoint_path=os.path.join(checkpoint_path,name)
checkpoint=model_checkpoint=keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_path,
    monitor='val_loss',
    mode='min',
    save_best_only=True,
    save_freq='epoch',
    verbose=1
)

early_stop=early_stopping=keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0,
    patience=3,
    verbose=1,
    mode="auto",
    baseline=None,
    restore_best_weights=True,
    start_from_epoch=0,
)

reduce_lr=keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.2,
    patience=6,
    verbose=0,
    mode="auto",
    min_delta=0.0001,
    min_lr=0.0,
)

# Callback to log training data to a CSV file
cnn_path='/content/FER_2013_Emotion_Detection/Custom_CNN_From_Scratch'
csv_logger = CSVLogger(os.path.join(cnn_path,'training.log'))

# Aggregating all callbacks into a list
callbacks = [checkpoint, early_stop, reduce_lr, csv_logger]  # Adjusted as per your use-case

train_steps_per_epoch = train_generator.samples // train_generator.batch_size + 1
validation_steps_epoch = validation_generator.samples // validation_generator.batch_size + 1
test_steps_epoch = test_generator.samples // test_generator.batch_size + 1

print(train_steps_per_epoch,validation_steps_epoch,test_steps_epoch)

history = model.fit(
    train_generator,
    steps_per_epoch=train_steps_per_epoch,
    epochs=40,
    validation_data=validation_generator,
    validation_steps=validation_steps_epoch,
    callbacks=callbacks)

def plot_training_history(history):
    """
    Plots the training and validation accuracy and loss.

    Parameters:
    - history: A Keras History object. Contains the logs from the training process.

    Returns:
    - None. Displays the matplotlib plots for training/validation accuracy and loss.
    """
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs_range = range(len(acc))

    plt.figure(figsize=(20, 5))

    # Plot training and validation accuracy
    plt.subplot(1, 2, 1)
    plt.plot(epochs_range, acc, label='Training Accuracy')
    plt.plot(epochs_range, val_acc, label='Validation Accuracy')
    plt.legend(loc='lower right')
    plt.title('Training and Validation Accuracy')

    # Plot training and validation loss
    plt.subplot(1, 2, 2)
    plt.plot(epochs_range, loss, label='Training Loss')
    plt.plot(epochs_range, val_loss, label='Validation Loss')
    plt.legend(loc='upper right')
    plt.title('Training and Validation Loss')

    plt.show()

plot_training_history(history)

"""# Model Evaluation"""

train_loss, train_accu = model.evaluate(train_generator)
test_loss, test_accu = model.evaluate(test_generator)
print("final train accuracy = {:.2f} , validation accuracy = {:.2f}".format(train_accu*100, test_accu*100))

predicted_values=model.predict(test_generator,steps=np.ceil(test_generator.samples/test_generator.batch_size))

predicted_values=np.argmax(predicted_values,axis=1)

predicted_values

true_classes = test_generator.classes

true_classes.shape

from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
from sklearn.utils.class_weight import compute_class_weight

# Generate the confusion matrix
cm = confusion_matrix(true_classes, predicted_values)
class_labels = list(test_generator.class_indices.keys())
# Plotting with seaborn
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt="d", cmap="Reds", xticklabels=class_labels, yticklabels=class_labels)
plt.title('Confusion Matrix')
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

"""Model is not Working Good this is happening due to class imbalance in our dataset disgust have the lowest dataset."""

# Printing the classification report
report = classification_report(true_classes,
                               predicted_values,
                               target_names=class_labels,
                               zero_division=0)
print("Classification Report:\n", report)

x=tf.expand_dims(test_generator[2][0][3].shape,axis=0)
x.shape

# Emotion classes for the dataset
Emotion_Classes = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']

# Assuming test_generator and model are already defined
batch_size = test_generator.batch_size

# Selecting a random batch from the test generator
Random_batch = np.random.randint(0, len(test_generator) - 1)

# Selecting random image indices from the batch
Random_Img_Index = np.random.randint(0, batch_size, 10)

# Setting up the plot
fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(10, 5),
                         subplot_kw={'xticks': [], 'yticks': []})

for i, ax in enumerate(axes.flat):
    # Fetching the random image and its label
    Random_Img = test_generator[Random_batch][0][Random_Img_Index[i]]
    Random_Img_Label = np.argmax(test_generator[Random_batch][1][Random_Img_Index[i]], axis=0)

    # Making a prediction using the model
    Model_Prediction = np.argmax(model.predict(tf.expand_dims(Random_Img, axis=0), verbose=0), axis=1)[0]

    # Displaying the image
    ax.imshow(Random_Img.squeeze(), cmap='gray')  # Assuming the images are grayscale
    # Setting the title with true and predicted labels, colored based on correctness
    color = "green" if Emotion_Classes[Random_Img_Label] == Emotion_Classes[Model_Prediction] else "red"
    ax.set_title(f"True: {Emotion_Classes[Random_Img_Label]}\nPredicted: {Emotion_Classes[Model_Prediction]}", color=color)

plt.tight_layout()
plt.show()

"""# Model 2 Using Augmentation"""

# Define paths to the train and validation directories
train_data_dir = '/content/train'
test_data_dir = '/content/test'
# validation_data_dir = '/content/emotion_detection_project/datasets/raw/test'

# Set some parameters
img_width, img_height = 48, 48  # Size of images
batch_size = 64
epochs = 10
num_classes = 7  # Update this based on the number of your classes

#Defining our Image Data Generator for making augmentation
data_generator = ImageDataGenerator(
    rescale=1./255,  # Rescale the pixel values from [0, 255] to [0, 1]
    #1
    rotation_range=40,  # Degree range for random rotations
    #2
    width_shift_range=0.2,  # Range (as a fraction of total width) for random horizontal shifts
    #3
    height_shift_range=0.2,  # Range (as a fraction of total height) for random vertical shifts
    #4
    shear_range=0.2,  # Shearing intensity (shear angle in counter-clockwise direction)
    #5
    zoom_range=0.2,  # Range for random zoom
    #6
    horizontal_flip=True,  # Randomly flip inputs horizontally
    #7
    fill_mode='nearest',  # Strategy to fill newly created pixels, which can appear after a rotation or a width/height shift
    validation_split=0.2  # Set the validation split; 20% of the data will be used for validation
)

test_data_generator=ImageDataGenerator(
    rescale=1./255
)

train_generator=data_generator.flow_from_directory(
    train_data_dir,
    target_size=(48, 48),
    color_mode='grayscale',
    class_mode='categorical',
    batch_size=batch_size,
    subset='training'
)
validation_generator=data_generator.flow_from_directory(
    train_data_dir,
    target_size=(48,48),
    color_mode='grayscale',
    classes=None,
    class_mode='categorical',
    batch_size=batch_size,
    subset='validation'
)
test_generator=test_data_generator.flow_from_directory(
    test_data_dir,
    target_size=(48,48),
    color_mode='grayscale',
    classes=None,
    class_mode='categorical',
    batch_size=batch_size,
)

checkpoint_path='/content/FER_2013_Emotion_Detection/Custom_CNN_With_Augmentation/'
name='Custom_CNN_Model_chkpt'
checpoint_path=os.path.join(checkpoint_path,name)
checkpoint2=model_checkpoint=keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_path,
    monitor='val_loss',
    mode='min',
    save_best_only=True,
    save_freq='epoch',
    verbose=1
)
early_stop=early_stopping=keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0,
    patience=3,
    verbose=1,
    mode="auto",
    baseline=None,
    restore_best_weights=True,
    start_from_epoch=0,
)
reduce_lr=keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.2,
    patience=6,
    verbose=0,
    mode="auto",
    min_delta=0.0001,
    min_lr=0.0,
)
csv_logger = CSVLogger(os.path.join('/content/FER_2013_Emotion_Detection/Custom_CNN_With_Augmentation/','training.log'))
callbacks2=[checkpoint2,early_stop,reduce_lr,csv_logger]

train_steps_per_epoch=np.ceil(train_generator.samples/batch_size)
print(train_steps_per_epoch)
validation_steps=np.ceil(validation_generator.samples/batch_size)
print(validation_steps)

history = model.fit(
    train_generator,
    steps_per_epoch=train_steps_per_epoch,
    epochs=40,
    validation_data=validation_generator,
    validation_steps=validation_steps,
    callbacks=callbacks2)

